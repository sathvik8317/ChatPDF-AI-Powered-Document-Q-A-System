# ChatPDF: RAG PDF Query App

A Streamlit-based application for querying PDF documents using a Retrieval-Augmented Generation (RAG) pipeline. This project leverages LangChain, a free vector database (Chroma), FastEmbedEmbeddings (using the `answerdotai/answerai-colbert-small-v1` model), and Ollama for large language model (LLM) inference.

---

## 📄 Table of Contents

* [🎯 Features](#-features)
* [🚀 Getting Started](#-getting-started)

  * [Prerequisites](#prerequisites)
  * [Installation](#installation)
  * [Downloading LLM Models](#downloading-llm-models)
* [💡 Usage](#-usage)

  * [Running the App](#running-the-app)
  * [Interacting with the Bot](#interacting-with-the-bot)
* [📁 Project Structure](#-project-structure)
* [⚙️ Configuration](#️-configuration)
* [🛠️ How It Works](#️-how-it-works)
* [🤝 Contributing](#-contributing)
* [📄 License](#-license)

---

## 🎯 Features

* **PDF Ingestion**: Upload one or multiple PDF files and automatically split them into overlapping text chunks.
* **Vector Embeddings**: Generate dense vector representations using `FastEmbedEmbeddings` with the `answerdotai/answerai-colbert-small-v1` model.
* **Free Vector Store**: Persist embeddings and chunks in a local Chroma index.
* **Retrieval & Reranking**: Retrieve top-K relevant chunks via similarity search and rerank them with an LLM-based scoring prompt.
* **Conversational QA**: Leverage a local Ollama LLM (`ChatOllama`) for answer generation from the highest-quality context.
* **Streamlit UI**: Clean and intuitive web interface powered by `streamlit_chat`.

---

## 🚀 Getting Started

### Prerequisites

* Python 3.8 or higher
* [Ollama CLI](https://ollama.com) installed and configured
* Internet access to download LLM models from Ollama

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/chatpdf-rag-app.git
cd chatpdf-rag-app

# Create and activate a virtual environment
python3 -m venv .venv
source .venv/bin/activate   # macOS/Linux
.\.venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

---

### Downloading LLM Models

This application uses LLMs provided by Ollama. Follow these steps to download the models:

```bash
# Authenticate with Ollama (if required)
ollama login

# List available models
ollama list

# Pull the desired model (e.g., qwen2.5:7b)
ollama pull qwen2.5:7b

# Verify installation
ollama list
```

Your model is now available locally for the application to use.

---

## 💡 Usage

### Running the App

```bash
streamlit run app.py
```

Open your browser to `http://localhost:8501` to start chatting with your PDFs.

### Interacting with the Bot

1. **Upload** one or more PDF files via the uploader widget.
2. The app will **ingest** each PDF and display ingestion time.
3. Enter your **questions** in the input box.
4. The bot retrieves relevant passages, reranks them, and returns a coherent answer.

---

## 📁 Project Structure

```
├── app.py           # Streamlit frontend
├── rag.py           # RAG pipeline implementation
├── requirements.txt # Python dependencies
└── README.md        # Project overview and instructions
```

---

## ⚙️ Configuration

* **LLM Model**: Defaults to `qwen2.5:7b`. To use a different model, set the `LLM_MODEL` environment variable:

  ```bash
  export LLM_MODEL="your-model-name"
  ```

* **Embedding Model**: Uses `answerdotai/answerai-colbert-small-v1`. To change, modify the `FastEmbedEmbeddings(model=...)` argument in `rag.py`.

* **Persist Directory**: Each ingestion creates a timestamped directory (`chroma_<timestamp>`) for the Chroma index. Delete old directories to free up space.

---

## 🛠️ How It Works

### Document Ingestion

1. **PyPDFLoader** reads PDFs and converts pages into `Document` objects.
2. **RecursiveCharacterTextSplitter** splits text into overlapping chunks (400 characters, 200 overlap).
3. **Chroma** builds or loads a local vector index of embeddings generated by `FastEmbedEmbeddings`.

### Chunk Scoring & Retrieval

1. **Similarity Search**: Retrieves the top 20 chunks via Chroma’s similarity search.
2. **LLM Scoring**: The LLM rates each chunk’s usefulness (1–10) for the query.
3. **Top-K Selection**: Selects the top 5 highest-scoring chunks.

### Answer Generation

1. **Context Assembly**: Concatenate selected chunks.
2. **Final Prompt**: Supply context and question to Ollama for answer generation.

---

## 🤝 Contributing

Contributions welcome! Open issues or pull requests for bugs, features, or improvements.

---

## 📄 License

MIT License. See [LICENSE](LICENSE) for details.
